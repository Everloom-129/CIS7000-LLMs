{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\" style=\"color:green;font-size: 3em;\">Homework 0:\n",
        "Exploring LLM Outputs For Different Models</h1>\n",
        "\n",
        "In this homework, you will explore the behavior and performance of different large language models (LLMs) by querying them with various prompts. You will compare the outputs from different models and provide insights into scenarios where one model significantly outperforms the other. This exercise will help you understand the strengths and weaknesses of various models and the impact of model size, specialization, and other factors on performance.\n",
        "\n",
        "**Instructions:**\n",
        "1. Use various methods to interact with and query different large language models. You can use online chatbot websites (e.g., [WebLLM](https://chat.webllm.ai/)), [ LMSYS chatbot arena](https://lmarena.ai/), code using the [Hugging Face library](https://huggingface.co/docs/transformers/en/llm_tutorial), and use tools like the [MT Bench Arena](https://huggingface.co/spaces/lmsys/mt-bench). Be sure to list the methods you use to access these models.\n",
        "\n",
        "2. For each model, generate outputs using a variety of prompts that test different aspects of the models, such as reasoning, creativity, factual recall, and conversational abilities.\n",
        "\n",
        "3. For each comparison, judge which model provides the better response. Document the prompts, the outputs, and your reasoning for choosing one model over the other.\n",
        "\n",
        "4. After completing all parts in the notebook, please submit a .ipynb file to Gradescope under the \"HW0\" assignment. Please note that all reasonable answers will be accepted and get full credit.\n",
        "\n",
        "\n",
        "# Part 1: Comparative Performance Analysis\n",
        "Identify scenarios where one model is significantly better than the other. Describe the prompt, the outputs from both models, and why you believe one model performed better.\n",
        "\n",
        "- **Example 1**: Provide an example where a larger model outperforms a smaller model. What aspects of the prompt, model architecture, or training process do you think contributed to this result?\n",
        "\n",
        "  **Your Answer:**\n",
        "\n",
        "  [Provide your answer here]\n",
        "\n",
        "- **Example 2**: Provide an example where a smaller model outperforms a larger model. Why might this occur?\n",
        "\n",
        "  **Your Answer:**\n",
        "\n",
        "  [Provide your answer here]\n",
        "\n",
        "- **Example 3**: Provide an example where a specialized model (e.g., a chat version of the model such as Llama-2-7b-chat) performs better than an unspecialized model. Discuss the factors that might lead to this result.\n",
        "\n",
        "  **Your Answer:**\n",
        "\n",
        "  [Provide your answer here]\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "hYS3WK2NfGFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Prompt Sensitivity\n",
        "Explore how different types of prompts (e.g., open-ended, factual, creative) affect the performance of the models. Provide examples and analyze why certain prompts might favor one model over another.\n",
        "\n",
        "**Your Answer:**\n",
        "\n",
        "[Provide your answer here]\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5wcU1Ui2fb4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Inference\n",
        "Based on your observations, what can you infer about the design and training of these models? How might the size, architecture, and training data of a model influence its performance on different tasks?\n",
        "\n",
        "**Your Answer:**\n",
        "\n",
        "[Provide your answer here]\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xWg6quoYfgyQ"
      }
    }
  ]
}